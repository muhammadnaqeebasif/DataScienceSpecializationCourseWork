{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "import urllib\n",
    "import pickle\n",
    "import math\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Glove Embedding Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"embedding.pickle\"):\n",
    "    \n",
    "    urllib.request.urlretrieve(\"https://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "                             \"glove.6B.zip\")\n",
    "    \n",
    "    with zipfile.ZipFile(\"glove.6B.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"embedding.pickle\"):\n",
    "    embeddings_index = dict()\n",
    "    with open('./glove/glove.6B.100d.txt',encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            values = line.split(\" \")\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs            \n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer.pickle\" , 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "vocab_size = len(tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists(\"embedding.pickle\")):\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "  \n",
    "    with open(\"embedding.pickle\",'wb') as file:\n",
    "        pickle.dump(embedding_matrix,file)\n",
    "\n",
    "else:\n",
    "    with open(\"embedding.pickle\",'rb') as file:\n",
    "        embedding_matrix = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"x_y_filtered.csv\"):\n",
    "    data = pd.read_csv(\"x_y_data.csv\")\n",
    "    ys = data.groupby('y')['x1'].count().reset_index()\n",
    "    ys = ys[ys.x1 >10]\n",
    "    data = data[data.y.isin(ys.y)]\n",
    "    y_vocab = data.y.max() + 1\n",
    "    data.to_csv(\"x_y_filtered.csv\",header=False,index=False)\n",
    "    with open(\"x_y_filtered_meta.pickle\",'wb') as file:\n",
    "        pickle.dump((y_vocab,total_size),file)\n",
    "    del ys\n",
    "    del data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"x_y_filtered_meta.pickle\",'rb') as file:\n",
    "    y_vocab,total_size = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_path):\n",
    "  \n",
    "    json_file = open(file_path + \".json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(file_path + \".h5\")\n",
    "\n",
    "    print(file_path + \" loaded.\")\n",
    "\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            63645600  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 4, 1024)           4608000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4, 1024)           8392704   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 4, 1024)           8392704   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 1024)              8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 53448)             3474120   \n",
      "=================================================================\n",
      "Total params: 98,652,712\n",
      "Trainable params: 35,007,112\n",
      "Non-trainable params: 63,645,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "file_path = \"Keras_Model/\"+\"model_\"\n",
    "for i in range(100):\n",
    "    if(not os.path.exists(file_path+ str(10*(i+1))+\"_epochs\"+\".json\")):\n",
    "        break\n",
    "\n",
    "if(i==0):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False))\n",
    "    model.add(LSTM(1024,return_sequences=True))\n",
    "    model.add(LSTM(1024,return_sequences=True))\n",
    "    model.add(LSTM(1024,return_sequences=True))\n",
    "    model.add(LSTM(1024))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(y_vocab, activation='softmax'))\n",
    "    print(\"Model created.\")\n",
    "else:\n",
    "    file_path = file_path + str(10*i) + \"_epochs\"\n",
    "    model = load_model(file_path)\n",
    "    \n",
    "print(model.summary())  \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create generators to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batch_size = 512):\n",
    "    start = 0\n",
    "    end = (start + batch_size) if (start +batch_size <= total_size) else total_size\n",
    "    \n",
    "    while True:\n",
    "        d = pd.read_csv(\"x_y_filtered.csv\",header=None,skiprows=start,nrows=(end-start))\n",
    "        X = d.iloc[:,:-1].values\n",
    "        y = to_categorical(d.iloc[:,-1].values,num_classes=y_vocab)\n",
    "        \n",
    "        \n",
    "        start = end if end < total_size else 0\n",
    "        end = (start + batch_size) if (start +batch_size <= total_size) else total_size\n",
    "            \n",
    "        yield X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,model_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(\"Keras_Model/\"+model_name+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"Keras_Model/\"+model_name+\".h5\")\n",
    "    print(\"Keras_Model/\"+model_name + \" saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 411s 411ms/step - loss: 7.0821 - acc: 0.0505\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 389s 389ms/step - loss: 6.8716 - acc: 0.0507\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 391s 391ms/step - loss: 6.8395 - acc: 0.0511\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 386s 386ms/step - loss: 6.7358 - acc: 0.0524\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 390s 390ms/step - loss: 6.6984 - acc: 0.0597\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 391s 391ms/step - loss: 6.6499 - acc: 0.0648\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 403s 403ms/step - loss: 6.5995 - acc: 0.0674\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 383s 383ms/step - loss: 6.5007 - acc: 0.0753\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 383s 383ms/step - loss: 6.4529 - acc: 0.0768\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 386s 386ms/step - loss: 6.4595 - acc: 0.0799\n",
      "Keras_Model/model_10_epochs saved to disk.\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 394s 394ms/step - loss: 6.3498 - acc: 0.0807\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 390s 390ms/step - loss: 6.3701 - acc: 0.0821\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 386s 386ms/step - loss: 6.3988 - acc: 0.0832\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 387s 387ms/step - loss: 6.3504 - acc: 0.0846\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 391s 391ms/step - loss: 6.3606 - acc: 0.0856\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 401s 401ms/step - loss: 6.3499 - acc: 0.0849\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 392s 392ms/step - loss: 6.3554 - acc: 0.0851\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 396s 396ms/step - loss: 6.3167 - acc: 0.0885\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 386s 386ms/step - loss: 6.3015 - acc: 0.0858\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 394s 394ms/step - loss: 6.3240 - acc: 0.0884\n",
      "Keras_Model/model_20_epochs saved to disk.\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 394s 394ms/step - loss: 6.2347 - acc: 0.0892\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 391s 391ms/step - loss: 6.2371 - acc: 0.0888\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 405s 405ms/step - loss: 6.2735 - acc: 0.0904\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 397s 397ms/step - loss: 6.2488 - acc: 0.0916\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 395s 395ms/step - loss: 6.2672 - acc: 0.0920\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 390s 390ms/step - loss: 6.2634 - acc: 0.0909\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 389s 389ms/step - loss: 6.2759 - acc: 0.0907\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 392s 392ms/step - loss: 6.2405 - acc: 0.0937\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 396s 396ms/step - loss: 6.2303 - acc: 0.0916\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 405s 405ms/step - loss: 6.2550 - acc: 0.0928\n",
      "Keras_Model/model_30_epochs saved to disk.\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 392s 392ms/step - loss: 6.1622 - acc: 0.0935\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 396s 396ms/step - loss: 6.1746 - acc: 0.0921\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 389s 389ms/step - loss: 6.2093 - acc: 0.0936\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 400s 400ms/step - loss: 6.2016 - acc: 0.0951\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 416s 416ms/step - loss: 6.2397 - acc: 0.0939\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 415s 415ms/step - loss: 6.2220 - acc: 0.0930\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 402s 402ms/step - loss: 6.2313 - acc: 0.0930\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 396s 396ms/step - loss: 6.1979 - acc: 0.0965\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 382s 382ms/step - loss: 6.1853 - acc: 0.0944\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 393s 393ms/step - loss: 6.2129 - acc: 0.0954\n",
      "Keras_Model/model_40_epochs saved to disk.\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 395s 395ms/step - loss: 6.1262 - acc: 0.0959\n",
      "Epoch 2/10\n",
      " 304/1000 [========>.....................] - ETA: 4:31 - loss: 6.1529 - acc: 0.0933"
     ]
    }
   ],
   "source": [
    "model_saved = i\n",
    "for i in range(model_saved,model_saved +10):\n",
    "    batch_size = 128\n",
    "    steps = 1000\n",
    "    model.fit_generator(generator(batch_size),steps,10)\n",
    "    save_model(model,\"model_\"+str(10*(i+1))+\"_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
