---
title: "Data Science Specialization Capstone Project - Week 2"
author: "Naqeeb Asif"
date: "4 December 2018"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synapsis
This document describes the data acquisition, basic summaries, cleaning, 
pre-processing  and basic exploratory analysis done in correspondence to the 
requirements in Data Science Capstone Projects.

## Acquisition of data and basic Summary
The data is downloaded from the [link](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)
provided in the instructions of the project. Following three US files are loaded
from the data provided:

* en_US.blogs.txt
* en_US.twitter.txt
* en_US.news.txt
```{r loadLibraries,echo=FALSE,warning=FALSE,message=FALSE}
```

```{r acquisition,echo=FALSE,results=FALSE,cache=TRUE,message=FALSE,warning=FALSE}
```

The basic summary of the data is as follows:

```{r basic_summary, echo=FALSE}
```

## Data Preprocessing
Following pre-processing is done on the data:

* Data is sampled as data is too large. Only 3 % of the data is taken from each 
data file.
* Corpus is generated
* Following transformations are performed:

    1. NonASCII characters are removed.
    2. Profanity words are removed.
    3. StopWords are removed.
    4. URLs are removed.
    5. Punctuations are removed.
    6. Numbers are removed.
    7. Words are uncapitalized.
    8. Spaces are removed.
  
* Creating DocumentTermMatrices:
    1. 1-Gram
    2. 2-Gram
    3. 3-Gram
    4. 4-Gram

```{r generating_corpus, echo=FALSE, cache=TRUE}
```


```{r clean_corpus,echo=FALSE,cache=TRUE}
```

```{r TermDocumentMatrix,echo=FALSE,cache=TRUE}
```

## Exploratory Analysis
In this section barplots and wordclouds are plotted for different d-grams model.

### 1-Gram
```{r one_d_gram_bar_plot, echo=FALSE}
```

```{r one_d_gram_word_cloud,echo=FALSE,warning=FALSE}
```

### 2-Gram
```{r two_d_gram_bar_plot, echo=FALSE}
```

```{r two_d_gram_word_cloud,echo=FALSE,warning=FALSE}
```

### 3-Gram
```{r three_d_gram_bar_plot, echo=FALSE}
```

```{r three_d_gram_word_cloud,echo=FALSE,warning=FALSE}
```

### 4-Gram
```{r four_d_gram_bar_plot, echo=FALSE}
```

```{r four_d_gram_word_cloud,echo=FALSE,warning=FALSE}
```

## Results
Following results can be observed from the above analysis:

1. In 1-gram analysis most of the frequent words are stop words. They were not removed as it would have affected the next steps.
2. In 2-gram the most frequent word is 'of the'.
3. In 3-gram the most frequent word is 'thanks for the'.
4. In 4-gram the most frequent word is 'thanks for the follow'
5. Some words may not appear in the word cloud plot even if they are the most frequent like for 4-gram as they won't fit in the plot.
  
## Future Plans
After the exploratory analysis, following are the further steps:

* Create predictive model using the techniques discussed above while taking into account the whole input. It can be done by processing the batches as 
the corpus would be too large to fit in the memory.
* Check modelling techniques for better performance.
* Create shiny application to make word prediction depending on the user input.

## Appendices
### 1. Appendix - Code for Loading Libraries
```{r loadLibraries,eval=FALSE}
library(NLP)
library(openNLP)
library(RWeka)
library(tidyverse)
library(knitr)
library(tm)
library(wordcloud)
```

### 2. Appendix - Code for Acquisition of Data
```{r acquisition,eval=FALSE}
ifelse(dir.exists("../data/"),FALSE,dir.create("../data"))
ifelse(!file.exists("../data/Coursera-SwiftKey.zip"),download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip","../data/Coursera-SwiftKey.zip"),FALSE)
ifelse(dir.exists("../data/final/"),FALSE,unzip("../data/Coursera-SwiftKey.zip",exdir = file.path("../data/"),overwrite = FALSE))
data_path = list(US_Blog = "../data/final/en_US/en_US.blogs.txt",
                 US_Twitter = "../data/final/en_US/en_US.twitter.txt",
                 US_News = "../data/final/en_US/en_US.news.txt")
data = map(data_path,readLines)
```

### 3. Appendix - Basic Summary of Data 
```{r basic_summary, eval=FALSE}
basic_summary <- map_dfr(data,
                          function(x){
                            list(lines = length(x),
                                 chars = sum(nchar(x)), 
                                 chars_longest_line = max(nchar(x)))
                          })
size = map_dbl(data_path, function(x){ 
                        file_info <-file.info(x)
                        file_info$size/(1024 * 1024)
                        })
basic_summary$size_mb <- size
basic_summary <- as.data.frame(basic_summary)
row.names(basic_summary) <- names(data_path)
basic_summary %>% kable()
```

### 4. Appedix - Code for Corpus Generation
```{r generating_corpus, eval=FALSE}
set.seed(1234)
sample_data <- map(data,function(x){
                              sample(x,length(x)*0.03)}
                       )
sample_data <- c(sample_data[['US_Blog']],sample_data[['US_Twitter']],
                  sample_data[['US_News']])

corpus = VCorpus(VectorSource(sample_data))
profanity <- readLines("https://www.cs.cmu.edu/~biglou/resources/bad-words.txt")
```

### 5. Appendix - Code for Corpus Transformation
```{r clean_corpus,eval=FALSE}
sub_transformer <- content_transformer(function (x , pattern ,replacement="") gsub(pattern,replacement, x))

# removeNonASCII
corpus <- tm_map(corpus,content_transformer(function(x)iconv(x, "latin1", "ASCII", sub="")))

# remove profanity words
corpus <- tm_map(corpus,removeWords,profanity)

# remove URLS
corpus <- tm_map(corpus,sub_transformer,"http[[:alnum:][:punct:]]*")

# removing punctuations
corpus <- tm_map(corpus,sub_transformer,"[[:punct:]]*")

# removing the digits
corpus <- tm_map(corpus,sub_transformer,"[[:digit:]]*")

# words are uncapitalized
corpus <- tm_map(corpus,
                 content_transformer(
                   function(x){ map_chr(x,tolower)
                              }
                   )
                 )

# spaces are removed
corpus <- tm_map(corpus,sub_transformer,"\\s+"," ")

```

### 6. Appendix - Code for creating DocumentTermMatrices
```{r TermDocumentMatrix,eval=FALSE}
create_ngram <- function(x){
                    gram <- function(y) NGramTokenizer(y, Weka_control(min = x, max = x))
                    dtm <- TermDocumentMatrix(corpus,control = list(tokenize = gram))
                    tidytext::tidy(dtm) %>% dplyr::group_by(term) %>% 
                      summarize(freq=sum(count)) %>% dplyr::arrange(desc(freq))
                 }

tdm <- map(list(unigram=1,bigram=2,trigram=3,fourgram=4),create_ngram)

```

### 7. Appendix - Code for creating barplot of 1-Gram
```{r one_d_gram_bar_plot, eval=FALSE}
unigram <- tdm$unigram 
g <- ggplot(unigram[1:20,],aes(x= reorder(term, -freq),y=freq)) + geom_bar(stat = "identity")
g + theme(axis.text.x = element_text(angle = 90, hjust = 1),plot.title = element_text(hjust = 0.5)) + 
  ggtitle("Bar Plot for 1-Gram ") + xlab("word")
```

### 8. Appendix - Code for creating wordcloud of 1-Gram
```{r one_d_gram_word_cloud,eval=FALSE}
wordcloud(words = unigram$term, freq = unigram$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

### 9. Appendix - Code for creating barplot of 2-Gram
```{r two_d_gram_bar_plot, eval=FALSE}
bigram <- tdm$bigram 
g <- ggplot(bigram[1:20,],aes(x= reorder(term, -freq),y=freq)) + geom_bar(stat = "identity")
g + theme(axis.text.x = element_text(angle = 90, hjust = 1),plot.title = element_text(hjust = 0.5)) + 
  ggtitle("Bar Plot for 2-Gram ") + xlab("word")
```

### 10. Appendix - Code for creating wordcloud of 2-Gram TDM
```{r two_d_gram_word_cloud,eval=FALSE}
wordcloud(words = bigram$term, freq = bigram$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

### 11. Appendix - Code for creating barplot of 3-Gram
```{r three_d_gram_bar_plot, eval=FALSE}
trigram <- tdm$trigram 
g <- ggplot(trigram[1:20,],aes(x= reorder(term, -freq),y=freq)) + geom_bar(stat = "identity")
g + theme(axis.text.x = element_text(angle = 90, hjust = 1),plot.title = element_text(hjust = 0.5)) + 
  ggtitle("Bar Plot for 3-Gram ") + xlab("word")
```

### 12. Appendix - Code for creating wordcloud of 3-Gram
```{r three_d_gram_word_cloud,eval=FALSE}
wordcloud(words = trigram$term, freq = trigram$freq, min.freq = 1,
          max.words=300, random.order=FALSE, rot.per=0.30, 
          colors=brewer.pal(8, "Dark2"))
```

### 13. Appendix - Code for creating barplot of 4-Gram
```{r four_d_gram_bar_plot, eval=FALSE}
fourgram <- tdm$fourgram 
g <- ggplot(fourgram[1:20,],aes(x= reorder(term, -freq),y=freq)) + geom_bar(stat = "identity")
g + theme(axis.text.x = element_text(angle = 90, hjust = 1),plot.title = element_text(hjust = 0.5)) + 
  ggtitle("Bar Plot for 4-Gram ") + xlab("word")
```

### 14. Appendix - Code for creating wordcloud of 4-Gram
```{r four_d_gram_word_cloud,eval=FALSE}
wordcloud(words = fourgram$term, freq = fourgram$freq, min.freq = 1,
          max.words=300, random.order=FALSE, rot.per=0.30, 
          colors=brewer.pal(8, "Dark2"))
```
